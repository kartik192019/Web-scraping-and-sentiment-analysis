{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNX7o3Cz5bBsBBFvTMGfVmq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-WthsG13nrZ1","executionInfo":{"status":"ok","timestamp":1710346976716,"user_tz":-330,"elapsed":4166,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","import re"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('vader_lexicon')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77P57sY2n6gP","executionInfo":{"status":"ok","timestamp":1710346985006,"user_tz":-330,"elapsed":1132,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}},"outputId":"38d119f4-837a-452b-9435-6993acf92c11"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["def scrape_website(url):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        title = soup.find('h1', class_='entry-title').text.strip()\n","        content = soup.find('div', class_='td-post-content tagdiv-type').text.strip()\n","        return title, content\n","    else:\n","        return None, None\n"],"metadata":{"id":"ciG72U9Nn9IL","executionInfo":{"status":"ok","timestamp":1710347023608,"user_tz":-330,"elapsed":9,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def clean_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(text)\n","    cleaned_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word.isalpha()]\n","    return cleaned_tokens"],"metadata":{"id":"iuK8xv1_oGzo","executionInfo":{"status":"ok","timestamp":1710347040599,"user_tz":-330,"elapsed":14,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def create_dictionary(file_path, sentiment_score):\n","    word_dict = {}\n","    with open(file_path, 'r', encoding='latin-1') as file:\n","        for line in file:\n","            word = line.strip()\n","            word_dict[word] = sentiment_score\n","    return word_dict"],"metadata":{"id":"rfBiCmOVoLFg","executionInfo":{"status":"ok","timestamp":1710347050227,"user_tz":-330,"elapsed":9,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def calculate_scores(text, pos_dict, neg_dict):\n","    positive_score = sum(pos_dict.get(word, 0) for word in text)\n","    negative_score = sum(neg_dict.get(word, 0) for word in text)\n","    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n","    polarity_score = max(min(abs(polarity_score), 1), -1)\n","    subjectivity_score = (positive_score + negative_score) / (len(text) + 0.000001)\n","    return positive_score, negative_score, polarity_score, subjectivity_score"],"metadata":{"id":"OHXjtrt-oNMd","executionInfo":{"status":"ok","timestamp":1710347060594,"user_tz":-330,"elapsed":11,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def count_syllables(word):\n","    vowels = 'aeiouy'\n","    count = 0\n","    previous_is_vowel = False\n","    for char in word.lower():\n","        if char in vowels and not previous_is_vowel:\n","            count += 1\n","            previous_is_vowel = True\n","        elif char not in vowels:\n","            previous_is_vowel = False\n","    if word.endswith('e') and count > 1:\n","        count -= 1\n","    count = max(count, 1)\n","    return count\n","\n","# Function to calculate readability using Gunning Fog index\n","def calculate_readability(text):\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","    avg_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n","    stop_words = set(stopwords.words('english'))\n","    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n","    num_complex_words = sum(1 for word in filtered_words if count_syllables(word) > 2)\n","    fog_index = 0.4 * (avg_sentence_length + (num_complex_words / len(words)) * 100)\n","    return avg_sentence_length, num_complex_words, fog_index\n","\n","# Function to count cleaned words in the text\n","def count_cleaned_words(text):\n","    words = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n","    return len(cleaned_words)\n","\n","# Function to calculate count of personal pronouns\n","def count_personal_pronouns(text):\n","    pattern = r'\\b(?:I|we|my|ours|us)\\b'\n","    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n","    return len(matches)"],"metadata":{"id":"efEfm_XloP-u","executionInfo":{"status":"ok","timestamp":1710347074765,"user_tz":-330,"elapsed":8,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Function to calculate average word length\n","def calculate_average_word_length(text):\n","    words = word_tokenize(text)\n","    total_characters = sum(len(word) for word in words)\n","    total_words = len(words)\n","    average_word_length = total_characters / total_words if total_words > 0 else 0\n","    return average_word_length\n","\n","# Function to perform sentiment analysis\n","def analyze_sentiment(text):\n","    sia = SentimentIntensityAnalyzer()\n","    sentiment_scores = sia.polarity_scores(text)\n","    return sentiment_scores\n","\n","def calculate_syllable_count_per_word(text):\n","    # Tokenize text into words\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n","\n","    # Calculate syllable count per word\n","    syllable_count_per_word = [count_syllables(word) for word in cleaned_words]\n","\n","    return syllable_count_per_word"],"metadata":{"id":"GbVSDKPuoTZs","executionInfo":{"status":"ok","timestamp":1710347114909,"user_tz":-330,"elapsed":9,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# List of website URLs to scrape\n","urls = [\n","    \"https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\",\n","    # Add more URLs here\n","]\n","\n","# Initialize lists to store scraped data\n","titles = []\n","contents = []\n","positive_scores = []\n","negative_scores = []\n","polarity_scores = []\n","subjectivity_scores = []\n","avg_sentence_lengths = []\n","num_complex_words_list = []\n","fog_indices = []\n","word_counts = []\n","syllable_counts_per_word_list = []\n","personal_pronouns_counts = []\n","average_word_lengths = []\n","sentiment_scores_list = []"],"metadata":{"id":"3E93xjHwodRm","executionInfo":{"status":"ok","timestamp":1710347192485,"user_tz":-330,"elapsed":724,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Load Positive and Negative dictionaries\n","positive_dict = create_dictionary('positive-words.txt', 1)\n","negative_dict = create_dictionary('negative-words.txt', -1)\n"],"metadata":{"id":"bnUuMybPov-e","executionInfo":{"status":"ok","timestamp":1710347645011,"user_tz":-330,"elapsed":447,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for url in urls:\n","    title, content = scrape_website(url)\n","    if title and content:\n","        # Clean the text\n","        cleaned_text = clean_text(content)\n","\n","        # Calculate scores\n","        positive_score, negative_score, polarity_score, subjectivity_score = calculate_scores(cleaned_text, positive_dict, negative_dict)\n","        avg_sentence_length, num_complex_words, fog_index = calculate_readability(content)\n","        total_cleaned_words = count_cleaned_words(content)\n","        syllable_counts_per_word = calculate_syllable_count_per_word(content)\n","        personal_pronouns_count = count_personal_pronouns(content)\n","        average_word_length = calculate_average_word_length(content)\n","        sentiment_scores = analyze_sentiment(content)\n","\n","        # Append scraped data to lists\n","        titles.append(title)\n","        contents.append(content)\n","        positive_scores.append(positive_score)\n","        negative_scores.append(negative_score)\n","        polarity_scores.append(polarity_score)\n","        subjectivity_scores.append(subjectivity_score)\n","        avg_sentence_lengths.append(avg_sentence_length)\n","        num_complex_words_list.append(num_complex_words)\n","        fog_indices.append(fog_index)\n","        word_counts.append(total_cleaned_words)\n","        syllable_counts_per_word_list.append(syllable_counts_per_word)\n","        personal_pronouns_counts.append(personal_pronouns_count)\n","        average_word_lengths.append(average_word_length)\n","        sentiment_scores_list.append(sentiment_scores)\n","\n","# Create a DataFrame to store the scraped data\n","df = pd.DataFrame({\n","    'Title': titles,\n","    'Content': contents,\n","    'Positive Score': positive_scores,\n","    'Negative Score': negative_scores,\n","    'Polarity Score': polarity_scores,\n","    'Subjectivity Score': subjectivity_scores,\n","    'Average Sentence Length': avg_sentence_lengths,\n","    'Number of Complex Words': num_complex_words_list,\n","    'Fog Index': fog_indices,\n","    'Word Count': word_counts,\n","    'Syllable Count Per Word': syllable_counts_per_word_list,\n","    'Personal Pronouns Count': personal_pronouns_counts,\n","    'Average Word Length': average_word_lengths,\n","    'Sentiment Scores': sentiment_scores_list\n","})\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('scraped_data.csv', index=False)\n","\n"],"metadata":{"id":"TmiB67w_o1It","executionInfo":{"status":"ok","timestamp":1710347783100,"user_tz":-330,"elapsed":425,"user":{"displayName":"Kartik Batchu","userId":"13858678398354197567"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bSCptQ8jrAaI"},"execution_count":null,"outputs":[]}]}